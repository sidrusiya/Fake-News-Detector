{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faafe14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 18:02:51.087498: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    preprocessed_tokens = [\n",
    "        token.lemma_ for token in doc if token.text.lower() not in stop_words\n",
    "        ]\n",
    "    return \" \".join([stemmer.stem(token) for token in preprocessed_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a48b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "# FRE SCORE\n",
    "from textstat import flesch_reading_ease\n",
    "# SENTIMENT ANALYSIS USING VADER\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#POS TAGGING\n",
    "import spacy\n",
    "#VECTORISING TEXT AND CREATING PIPELINE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "#COSINE SIMILARITY BETWEEN REVIEWS\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caeae7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"WELFake_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9959fbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'title', 'text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c85e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RENAME COLUMNS\n",
    "df.rename(columns={'text': 'NEWS TEXT'}, inplace=True)\n",
    "df.rename(columns={'title': 'NEWS TITLE'}, inplace=True)\n",
    "df.rename(columns={'label': 'LABEL'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d39cb4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['PREPROCESSED TEXT'] = df['NEWS TEXT'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09e8504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_readability_score(df):\n",
    "    df['READABILITY_FRE'] = df['NEWS TEXT'].apply(\n",
    "        lambda d: flesch_reading_ease(d))\n",
    "\n",
    "def add_title_length(df):\n",
    "    df['TITLE_LENGTH'] = df['NEWS TITLE'].apply(lambda d: len(d))\n",
    "\n",
    "def add_text_length(df):\n",
    "    df['TEXT LENGTH'] = df['NEWS TEXT'].apply(lambda d: len(d))\n",
    "\n",
    "def add_vader_text_sentiment_score(df):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    df['TEXT SENTIMENT SCORE'] = df['NEWS TEXT'].apply(\n",
    "        lambda d: sid.polarity_scores(d)['compound'])\n",
    "    \n",
    "def add_vader_title_sentiment_score(df):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    df['TITLE SENTIMENT SCORE'] = df['NEWS TITLE'].apply(\n",
    "        lambda d: sid.polarity_scores(d)['compound'])\n",
    "    \n",
    "def add_sentiment_category(df, threshold):\n",
    "\n",
    "    def assign_sentiment_category(score):\n",
    "        if score > threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['SENTIMENT CATEGORY'] = df['TEXT SENTIMENT SCORE'].apply(\n",
    "        assign_sentiment_category)\n",
    "    \n",
    "def add_count_punctuation(df):\n",
    "    def count_punctuation(text):\n",
    "        punctuation_count = sum([1 for char in text if char in string.punctuation])\n",
    "        return punctuation_count\n",
    "    \n",
    "    # Assuming you have a DataFrame called 'df' with a column 'REVIEW_TEXT'\n",
    "    df['TEXT PUNCTUATION COUNT'] = df['NEWS TEXT'].apply(count_punctuation)\n",
    "\n",
    "def add_count_capital_chars(df):\n",
    "    def count_capital_chars(text):\n",
    "        capital_count = sum([1 for char in text if char.isupper()])\n",
    "        return capital_count\n",
    "\n",
    "\n",
    "    # Assuming you have a DataFrame called 'df' with a column 'REVIEW_TEXT'\n",
    "    df['TEXT CAPITAL CHARS'] = df['NEWS TEXT'].apply(count_capital_chars)\n",
    "\n",
    "def add_pos_tags(df):\n",
    "    def count_pos(Pos_counts, pos_type):\n",
    "        pos_count = Pos_counts.get(pos_type, 0)\n",
    "        return pos_count\n",
    "\n",
    "    def pos_counts(text):\n",
    "        doc = nlp(text)\n",
    "        Pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "        return Pos_counts\n",
    "\n",
    "    poscounts =  df[\"NEWS TEXT\"].apply(pos_counts)\n",
    "    df['TEXT NUM NOUNS'] = df['NEWS TEXT'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.NOUN))\n",
    "    df['TEXT NUM VERBS'] = df['NEWS TEXT'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.VERB))\n",
    "    df['TEXT NUM ADJECTIVES'] = df['NEWS TEXT'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.ADJ))\n",
    "    df['TEXT NUM ADVERBS'] = df['NEWS TEXT'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.ADV))\n",
    "    \n",
    "def add_named_entities(df):\n",
    "    def count_entities(text):\n",
    "        doc = nlp(text)\n",
    "        ent_count = len([ent.text for ent in doc.ents])\n",
    "        return ent_count\n",
    "\n",
    "    df['TEXT NUM NAMED ENTITIES'] = df['NEWS TEXT'].apply(count_entities)\n",
    "    \n",
    "def calculate_lexical_diversity(text):\n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "    unique_words = len(set(words))\n",
    "    \n",
    "    if total_words > 0:\n",
    "        lexical_diversity = unique_words / total_words\n",
    "    else:\n",
    "        lexical_diversity = 0.0\n",
    "    \n",
    "    return lexical_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29135761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "NEWS TITLE    558\n",
       "NEWS TEXT      39\n",
       "LABEL           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e781253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE NULL RECORDS\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca476cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE ONLY 20,000 RECORDS\n",
    "df1 = df[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3f0c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/ipykernel_65632/88854239.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['TEXT WORD COUNT'] = df1['NEWS TEXT'].apply(lambda x: len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "#ADD COLUMN FOR NUMBER OF WORDS IN TEXT\n",
    "df1['TEXT WORD COUNT'] = df1['NEWS TEXT'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3ecef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/ipykernel_65632/1390510365.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1[\"TITLE WORD COUNT\"] = df1[\"NEWS TITLE\"].apply(lambda x: len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "#ADD COLUMN FOR NUMBER OF WORDS IN TITLE\n",
    "df1[\"TITLE WORD COUNT\"] = df1[\"NEWS TITLE\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d6f215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/ipykernel_65632/3849230110.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['TEXT LENGTH'] = df['NEWS TEXT'].apply(lambda d: len(d))\n",
      "/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/ipykernel_65632/3849230110.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['TITLE_LENGTH'] = df['NEWS TITLE'].apply(lambda d: len(d))\n"
     ]
    }
   ],
   "source": [
    "#ADD TEXT LENGTH AND TITLE LENGTH\n",
    "add_text_length(df1)\n",
    "add_title_length(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0cf4c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/ipykernel_65632/3849230110.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['TEXT SENTIMENT SCORE'] = df['NEWS TEXT'].apply(\n"
     ]
    }
   ],
   "source": [
    "#ADD SENTIMENT SCORE OF TEXT\n",
    "add_vader_text_sentiment_score(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "188beefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'NEWS TITLE', 'NEWS TEXT', 'LABEL', 'TEXT WORD COUNT',\n",
       "       'TITLE WORD COUNT', 'TEXT LENGTH', 'TITLE_LENGTH',\n",
       "       'TEXT SENTIMENT SCORE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e173740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/ipykernel_65632/3849230110.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['TITLE SENTIMENT SCORE'] = df['NEWS TITLE'].apply(\n"
     ]
    }
   ],
   "source": [
    "#ADD VADER SENTIMENT SCORE OF TITLE\n",
    "add_vader_title_sentiment_score(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0ac3ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/ipykernel_65632/3849230110.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['READABILITY_FRE'] = df['NEWS TEXT'].apply(\n"
     ]
    }
   ],
   "source": [
    "#ADD READABILITY SCORE OF TEXT\n",
    "add_readability_score(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d7fce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/ipykernel_65632/3849230110.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['TEXT CAPITAL CHARS'] = df['NEWS TEXT'].apply(count_capital_chars)\n"
     ]
    }
   ],
   "source": [
    "#ADD CAPITAL CHARACTER COUNT\n",
    "add_count_capital_chars(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7733e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/ps_r0r591fx137cf06vhn3tm0000gn/T/ipykernel_65632/3849230110.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['TEXT PUNCTUATION COUNT'] = df['NEWS TEXT'].apply(count_punctuation)\n"
     ]
    }
   ],
   "source": [
    "#ADD PUNCTUATION COUNT\n",
    "add_count_punctuation(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91d5c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pos_tags(df):\n",
    "    def count_pos(Pos_counts, pos_type):\n",
    "        pos_count = Pos_counts.get(pos_type, 0)\n",
    "        return pos_count\n",
    "\n",
    "    def pos_counts(text):\n",
    "        doc = nlp(text)\n",
    "        Pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "        return Pos_counts\n",
    "\n",
    "    poscounts =  df[\"NEWS TEXT\"].apply(pos_counts)\n",
    "    df['TEXT NUM NOUNS'] = df['NEWS TEXT'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.NOUN))\n",
    "    df['TEXT NUM VERBS'] = df['NEWS TEXT'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.VERB))\n",
    "    df['TEXT NUM ADJECTIVES'] = df['NEWS TEXT'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.ADJ))\n",
    "    df['TEXT NUM ADVERBS'] = df['NEWS TEXT'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.ADV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9d2af97",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#ADD POS TAGS\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43madd_pos_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m, in \u001b[0;36madd_pos_tags\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      8\u001b[0m     Pos_counts \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mcount_by(spacy\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mPOS)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Pos_counts\n\u001b[0;32m---> 11\u001b[0m poscounts \u001b[38;5;241m=\u001b[39m  \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNEWS TEXT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_counts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT NUM NOUNS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEWS TEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m text: count_pos(poscounts, spacy\u001b[38;5;241m.\u001b[39mparts_of_speech\u001b[38;5;241m.\u001b[39mNOUN))\n\u001b[1;32m     14\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT NUM VERBS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEWS TEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m text: count_pos(poscounts, spacy\u001b[38;5;241m.\u001b[39mparts_of_speech\u001b[38;5;241m.\u001b[39mVERB))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m, in \u001b[0;36madd_pos_tags.<locals>.pos_counts\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_counts\u001b[39m(text):\n\u001b[0;32m----> 7\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     Pos_counts \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mcount_by(spacy\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mPOS)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Pos_counts\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/language.py:1019\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1019\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:253\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:274\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/ml/tb_framework.py:33\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 33\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munseen_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_upper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/ml/parser_model.pyx:220\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/ml/parser_model.pyx:359\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.precompute_hiddens.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/ml/_precomputable_affine.py:27\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Preallocate array for layer output, including padding.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m Yf \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc2f(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, nF \u001b[38;5;241m*\u001b[39m nO \u001b[38;5;241m*\u001b[39m nP, zeros\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnF\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnO\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnI\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mYf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m Yf \u001b[38;5;241m=\u001b[39m Yf\u001b[38;5;241m.\u001b[39mreshape((Yf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nF, nO, nP))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Set padding. Padding has shape (1, nF, nO, nP). Unfortunately, we cannot\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# change its shape to (nF, nO, nP) without breaking existing models. So\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# we'll squeeze the first dimension here.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ADD POS TAGS\n",
    "add_pos_tags(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc599a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD COUNT OF NAMES ENTITIES IN TEXT\n",
    "add_named_entities(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97741a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD SENTIMENT CATEGORY OF TEXT\n",
    "add_sentiment_category(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a3f9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE AS CSV FILE\n",
    "\n",
    "df1.to_csv(\"training_data_set_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97518f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD LEXICAL DIVERSITY\n",
    "df['TEXT LEXICAL DIVERSITY'] = df['NEWS TEXT'].apply(calculate_lexical_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cacee092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN MODELS\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88233ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kpandey/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    }
   ],
   "source": [
    "classifiers = {}\n",
    "classifiers.update({\"XGBClassifier\": XGBClassifier(eval_metric='logloss',objective='binary:logistic',use_label_encoder=False)})\n",
    "# classifiers.update({\"CatBoostClassifier\": CatBoostClassifier(silent=True)})\n",
    "classifiers.update({\"LinearSVC\": LinearSVC(max_iter=10000)})\n",
    "# classifiers.update({\"MultinomialNB\": MultinomialNB()})\n",
    "# classifiers.update({\"LGBMClassifier\": LGBMClassifier()})\n",
    "classifiers.update({\"RandomForestClassifier\": RandomForestClassifier()})\n",
    "classifiers.update({\"DecisionTreeClassifier\": DecisionTreeClassifier()})\n",
    "classifiers.update({\"ExtraTreeClassifier\": ExtraTreeClassifier()})\n",
    "classifiers.update({\"AdaBoostClassifier\": AdaBoostClassifier()})\n",
    "classifiers.update({\"KNeighborsClassifier\": KNeighborsClassifier()})\n",
    "classifiers.update({\"RidgeClassifier\": RidgeClassifier()})\n",
    "classifiers.update({\"SGDClassifier\": SGDClassifier()})\n",
    "classifiers.update({\"BaggingClassifier\": BaggingClassifier()})\n",
    "classifiers.update({\"BernoulliNB\": BernoulliNB()})\n",
    "classifiers.update({\"LogisticRegression\": LogisticRegression()})\n",
    "classifiers.update({\"SVM\": SVC()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53598efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7299a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_numeric = [\n",
    "    'TEXT WORD COUNT', 'TITLE WORD COUNT', 'TEXT LENGTH', 'TITLE_LENGTH',\n",
    "       'TEXT SENTIMENT SCORE', 'TITLE SENTIMENT SCORE', 'READABILITY_FRE',\n",
    "       'TEXT CAPITAL CHARS', 'TEXT PUNCTUATION COUNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "817d61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1[features_numeric]\n",
    "Y = df1[\"LABEL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35431a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0304a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25a32f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kpandey/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     model run_time  accuracy  precision  f1_score\n",
      "0            XGBClassifier     0.01   0.80050   0.823795  0.804412\n",
      "2   RandomForestClassifier     0.07   0.79400   0.813181  0.799318\n",
      "9        BaggingClassifier     0.02   0.78475   0.820700  0.784804\n",
      "12                     SVM     0.25   0.77275   0.822660  0.767816\n",
      "5       AdaBoostClassifier     0.01   0.74825   0.787042  0.746411\n",
      "6     KNeighborsClassifier     0.01   0.74500   0.791485  0.739796\n",
      "3   DecisionTreeClassifier      0.0   0.71875   0.742081  0.724062\n",
      "8            SGDClassifier      0.0   0.69900   0.774534  0.674419\n",
      "11      LogisticRegression      0.0   0.69525   0.722024  0.698640\n",
      "4      ExtraTreeClassifier      0.0   0.69350   0.712945  0.701848\n",
      "7          RidgeClassifier      0.0   0.69175   0.722772  0.692288\n",
      "1                LinearSVC     0.19   0.69125   0.720413  0.693015\n",
      "10             BernoulliNB      0.0   0.66425   0.700377  0.659742\n"
     ]
    }
   ],
   "source": [
    "# CREATE A DATAFRAME OF MODELS WITH RUN TIME AND AUC SCORES\n",
    "df_models = pd.DataFrame(\n",
    "    columns=['model', 'run_time', 'accuracy', 'precision', 'f1_score'])\n",
    "\n",
    "for key in classifiers:\n",
    "    # STARTING TIME\n",
    "    start_time = time.time()\n",
    "    # CURRENT CLASSIFIER\n",
    "    clf = classifiers[key]\n",
    "    #TRAIN CLASSIFIER ON TRAINING DATA\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    # MAKE PREDICTIONS USING CURRENT CLASSIFIER\n",
    "    predictions = clf.predict(X_test_scaled)\n",
    "    # CALCULATE ACCURACY\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    f1score = f1_score(y_test, predictions)\n",
    "    \n",
    "    row = {'model': key,\n",
    "           'run_time': format(round((time.time() - start_time)/60, 2)),\n",
    "           'accuracy': accuracy,\n",
    "           'precision': precision,\n",
    "           'f1_score': f1score\n",
    "           }\n",
    "\n",
    "    df_models = df_models._append(row, ignore_index=True)\n",
    "\n",
    "df_models = df_models.sort_values(by='accuracy', ascending=False)\n",
    "\n",
    "# PRINT THE MODELS WITH AUC SCORES [SORTED]\n",
    "print(df_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66e52b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "dictionary[\"MODEL\"] = classifiers[\"XGBClassifier\"]\n",
    "\n",
    "dictionary[\"SCALER\"] = scaler\n",
    "\n",
    "with open(\"model_1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(dictionary, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f064b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
